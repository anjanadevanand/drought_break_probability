{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a913eefa-bb92-4dc2-a989-834c7c7900dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions to get daily P, E, Q\n",
    "\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import os\n",
    "import datetime\n",
    "import math\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def get_awra_var(var_name, awra_dir = '/g/data/fj8/BoM/AWRA/DATA/SCHEDULED-V6/', lat_slice = None, lon_slice = None, time_slice = None):\n",
    "    file_names = var_name + '_*.nc' \n",
    "    ds = xr.open_mfdataset(awra_dir + file_names, chunks = {'lat':400,'lon':400})\n",
    "\n",
    "    if lat_slice is None:\n",
    "        if time_slice is None:\n",
    "            da_var = ds[var_name].rename({'latitude':'lat','longitude':'lon'})\n",
    "        else:\n",
    "            da_var = ds[var_name].sel(time = time_slice).rename({'latitude':'lat','longitude':'lon'})\n",
    "    else:\n",
    "        if time_slice is None:\n",
    "            da_var = ds[var_name].sel(latitude = lat_slice, longitude = lon_slice).rename({'latitude':'lat','longitude':'lon'})\n",
    "        else:\n",
    "            da_var = ds[var_name].sel(time = time_slice, latitude = lat_slice, longitude = lon_slice).rename({'latitude':'lat','longitude':'lon'})      \n",
    "    return da_var\n",
    "\n",
    "def get_agcd_var(agcd_dir = '/g/data/zv2/agcd/v1/precip/total/r005/01day/', agcd_files = 'agcd_v1_precip_total_r005_daily_*.nc', \n",
    "                   lat_slice = slice(-44, -10), lon_slice = slice(112, 154), time_slice = None):\n",
    "    ds_agcd = xr.open_mfdataset(agcd_dir + agcd_files) #, chunks = {'lat':400,'lon':400})\n",
    "    if time_slice is None:\n",
    "        da_P = ds_agcd['precip'].sel(lat = lat_slice, lon = lon_slice)\n",
    "    else:\n",
    "        da_P = ds_agcd['precip'].sel(lat = lat_slice, lon = lon_slice, time = time_slice)\n",
    "    return da_P\n",
    "\n",
    "def calc_daily_PmEQ_roll(window, out_dir = None, lat_slice_P = None, lat_slice_EQ = None, lon_slice = None, time_slice = None):\n",
    "    \n",
    "    # read the data from orig files\n",
    "    if lat_slice_P is None:\n",
    "        if time_slice is None:\n",
    "            da_P = get_agcd_var()\n",
    "        else:\n",
    "            da_P = get_agcd_var(time_slice = time_slice)\n",
    "    else:\n",
    "        if time_slice is None:\n",
    "            da_P = get_agcd_var(lat_slice = lat_slice_P, lon_slice = lon_slice)\n",
    "        else:\n",
    "            da_P = get_agcd_var(lat_slice = lat_slice_P, lon_slice = lon_slice, time_slice = time_slice)\n",
    "\n",
    "    if lat_slice_EQ is None:\n",
    "        if time_slice is None:\n",
    "            da_E = get_awra_var(\"etot\")\n",
    "            da_Q = get_awra_var(\"qtot\")\n",
    "        else:\n",
    "            da_E = get_awra_var(\"etot\", time_slice = time_slice)\n",
    "            da_Q = get_awra_var(\"qtot\", time_slice = time_slice)\n",
    "    else:\n",
    "        if time_slice is None:\n",
    "            da_E = get_awra_var(\"etot\", lat_slice = lat_slice_EQ, lon_slice = lon_slice)\n",
    "            da_Q = get_awra_var(\"qtot\", lat_slice = lat_slice_EQ, lon_slice = lon_slice)\n",
    "        else:\n",
    "            da_E = get_awra_var(\"etot\", lat_slice = lat_slice_EQ, lon_slice = lon_slice, time_slice = time_slice)\n",
    "            da_Q = get_awra_var(\"qtot\", lat_slice = lat_slice_EQ, lon_slice = lon_slice, time_slice = time_slice)\n",
    "\n",
    "    time_new = da_P['time'].dt.floor('D')\n",
    "    da_P = da_P.assign_coords(time=time_new)\n",
    "    \n",
    "    # converting the datatypes of E to match P\n",
    "    lat_new = np.float32(da_E['lat'])\n",
    "    lon_new = np.float32(da_E['lon'])\n",
    "    da_E = da_E.assign_coords(lat = lat_new)\n",
    "    da_E = da_E.assign_coords(lon = lon_new)\n",
    "    lat_new = np.float32(da_Q['lat'])\n",
    "    lon_new = np.float32(da_Q['lon'])\n",
    "    da_Q = da_Q.assign_coords(lat=lat_new)\n",
    "    da_Q = da_Q.assign_coords(lon=lon_new)\n",
    "    \n",
    "    # return da_P, da_E, da_Q\n",
    "    \n",
    "    da_PmEQ = (da_P - da_E - da_Q).rename('PminusEQ')\n",
    "    \n",
    "    window_centre = math.floor(window/2)\n",
    "    daydiff = np.timedelta64(window_centre + 1, 'D')  # add 1 because these are accumulated variables. \n",
    "                                                      # value at one date is actually the accumulated amount upto that date (i.e., accumultaion from the previous date)\n",
    "    da_time_new = da_PmEQ.time - daydiff\n",
    "    \n",
    "    da_PmEQ_roll_temp = da_PmEQ.rolling(time=window, center=True).sum().assign_coords({'time': da_time_new})\n",
    "    da_PmEQ_roll = da_PmEQ_roll_temp[(window_centre + 1):(len(da_time_new) - window_centre + 1),:,:]\n",
    "    if out_dir is None:\n",
    "        return da_PmEQ_roll\n",
    "    else:\n",
    "        for year, sample in tqdm(da_PmEQ_roll.groupby('time.year')):\n",
    "            out_file = out_dir + 'PminusEQ_daily_roll_' + str(window) + 'days_' + str(year) + '.nc'\n",
    "            sample.to_netcdf(out_file)\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18b1f80-c788-415e-97ab-488157d123d8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/g/data/hh5/public/apps/miniconda3/envs/analysis3-21.10/lib/python3.9/site-packages/xarray/core/indexing.py:1226: PerformanceWarning: Slicing is producing a large chunk. To accept the large\n",
      "chunk and silence this warning, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
      "    ...     array[indexer]\n",
      "\n",
      "To avoid creating the large chunks, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': True}):\n",
      "    ...     array[indexer]\n",
      "  return self.array[key]\n",
      "/g/data/hh5/public/apps/miniconda3/envs/analysis3-21.10/lib/python3.9/site-packages/xarray/core/indexing.py:1226: PerformanceWarning: Slicing is producing a large chunk. To accept the large\n",
      "chunk and silence this warning, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
      "    ...     array[indexer]\n",
      "\n",
      "To avoid creating the large chunks, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': True}):\n",
      "    ...     array[indexer]\n",
      "  return self.array[key]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a6c695f45654bb284e287ff1914f15b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/110 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/g/data/hh5/public/apps/miniconda3/envs/analysis3-21.10/lib/python3.9/site-packages/xarray/core/indexing.py:1226: PerformanceWarning: Slicing is producing a large chunk. To accept the large\n",
      "chunk and silence this warning, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
      "    ...     array[indexer]\n",
      "\n",
      "To avoid creating the large chunks, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': True}):\n",
      "    ...     array[indexer]\n",
      "  return self.array[key]\n",
      "/g/data/hh5/public/apps/miniconda3/envs/analysis3-21.10/lib/python3.9/site-packages/xarray/core/indexing.py:1226: PerformanceWarning: Slicing is producing a large chunk. To accept the large\n",
      "chunk and silence this warning, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
      "    ...     array[indexer]\n",
      "\n",
      "To avoid creating the large chunks, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': True}):\n",
      "    ...     array[indexer]\n",
      "  return self.array[key]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4ab325addaf4215a541b35217fb22bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/110 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lat_slice_P = slice(-39, -32)\n",
    "lat_slice_EQ = slice(-32, -39)\n",
    "lon_slice = slice(139, 152)\n",
    "time_slice = slice('1911-01-01', '2020-05-31')\n",
    "\n",
    "for ts in [6, 12]:\n",
    "    window = ts*7\n",
    "    da_PmEQ_roll = calc_daily_PmEQ_roll(window = window, lat_slice_P = lat_slice_P, lat_slice_EQ = lat_slice_EQ, lon_slice = lon_slice, time_slice = time_slice)\n",
    "\n",
    "    main_dir = '/g/data/w97/ad9701/p_prob_analysis/temp_files/'\n",
    "    out_dir = main_dir + 'GLM_results_full_record/validation/PminusEQ_week2_roll_daily/'\n",
    "\n",
    "    for year, sample in tqdm(da_PmEQ_roll.groupby('time.year')):\n",
    "        out_file = out_dir + 'PminusEQ_daily_roll_' + str(ts) + 'weeks_' + str(year) + '.nc'\n",
    "        sample.to_netcdf(out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34f34dc-4c07-44ed-a14d-18912208a30e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:analysis3-21.10]",
   "language": "python",
   "name": "conda-env-analysis3-21.10-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
