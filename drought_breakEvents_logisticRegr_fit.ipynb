{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b79301-0cb1-4daa-826a-4050d9dd71d0",
   "metadata": {},
   "source": [
    "### Fit logistic regression to responses against predictors for selected sample grids\n",
    "\n",
    "#### Data used\n",
    "\n",
    "- Predictands: gridded precipitation data (AGCD v1), gridded evapotranspiration and runoff data (AWRA)\n",
    "- Predictors: season, climate drivers (ENSO, IOD, and SAM, these could be categorical or quantitative)\n",
    "\n",
    "#### Code fits the model to data at various time scales and thresholds, and creates summary plots to visualise results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fcee17ca-bc53-41d7-8a3b-2ec27cfc5ecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interactive namespace is empty.\n"
     ]
    }
   ],
   "source": [
    "%who"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e38d7ae-2309-4db9-96a1-910ea4127ad9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])?  y\n"
     ]
    }
   ],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09afaefa-1419-432f-876f-47766b8b3645",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fitting logistic regression models to test grid points\n",
    "\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statsmodels.formula.api import glm\n",
    "import statsmodels.api as sm\n",
    "# model = glm(formula, data, family)\n",
    "\n",
    "out_dir = '/g/data/w97/ad9701/p_prob_analysis/temp_files/'\n",
    "\n",
    "varname = 'PminusE' #'P'   # the name of the directory and file\n",
    "vname = 'PminusE' #'precip'  # the name of the variable inside the files\n",
    "fname = varname + '_*_SEA_*.nc'\n",
    "\n",
    "# select some thresholds to look at\n",
    "threshList = [50, 100, 150]\n",
    "\n",
    "# select some lat-lons to look at\n",
    "latList = [-34, -34, -34, -37, -37, -37]\n",
    "lonList = [148, 145, 142, 148, 145, 142]\n",
    "\n",
    "# select some timescales for analysis\n",
    "ts = [2, 6, 8, 12]\n",
    "\n",
    "# get the sst predictors\n",
    "sst_dir = '/g/data/w97/ad9701/p_prob_analysis/sst_data/'\n",
    "pNames = ['soi', 'sami', 'dmi', 'nino34_anom', 'nino4_anom']\n",
    "pFiles = ['soi_monthly.nc', 'newsam.1957.2021.nc', 'dmi.had.long.data.nc', 'nino34.long.anom.data.nc', 'nino4.long.anom.data.nc']\n",
    "for p in np.arange(len(pNames)):\n",
    "    ds_temp = xr.open_dataset(sst_dir+pFiles[p])\n",
    "    if (p>0):\n",
    "        ds_p[pNames[p]]=ds_temp[pNames[p]]\n",
    "    else:\n",
    "        ds_p = ds_temp\n",
    "    del ds_temp\n",
    "\n",
    "# select the predictors to include in the model\n",
    "predSel = ['season', 'soi', 'dmi']\n",
    "formula = 'response ~ C(season)+soi+dmi'\n",
    "    \n",
    "# function to create a new data frame that will be used to 'predict' probabilities from the fitted model\n",
    "# the new data points would cover combinations of unique values for categorical predictors and mean/perturbations one sd above the mean for quantitative predictors\n",
    "import itertools\n",
    "import pandas as pd\n",
    "def createNewDf(df, fields):\n",
    "    '''Function creates a sample dataframe from a larger input dataframe (df).\n",
    "       The sample points will include all permutations of columns specfied (fields).\n",
    "       String columns: use unique values. Numeric columns: Mean, Mean-1SD, Mean+1SD\n",
    "    '''\n",
    "    dataVal = []\n",
    "    for f in fields:\n",
    "        # str data types are assumed to be categorical variables\n",
    "        if (isinstance(df[f][0], str)):\n",
    "            dataVal.append(pd.unique(df[f]))\n",
    "        else:\n",
    "            temp = [df[f].mean()]\n",
    "            temp.extend([df[f].mean()+df[f].std(), df[f].mean()-df[f].std()])\n",
    "            dataVal.append(temp)\n",
    "            del temp\n",
    "    # get all combinations of values across the fields\n",
    "    dataValPermute = list(itertools.product(*dataVal))\n",
    "    # make it into a data frame\n",
    "    newDf = pd.DataFrame(dataValPermute, columns = fields)\n",
    "    return(newDf)\n",
    "\n",
    "def addLatLonTh(mydict, latSel, lonSel, threshSel):\n",
    "    mydict.update({'lat':latSel})\n",
    "    mydict.update({'lon':lonSel})\n",
    "    mydict.update({'threshold':threshSel})\n",
    "    return(mydict)\n",
    "\n",
    "def addLatLonThTimeDf(df, latSel, lonSel, threshSel, tsSel):\n",
    "    df['lat'] = latSel\n",
    "    df['lon'] = lonSel\n",
    "    df['threshold'] = threshSel\n",
    "    df['timescale'] = tsSel\n",
    "    return(df)\n",
    "\n",
    "lgR_params_list = []\n",
    "lgR_pvalues_list = []\n",
    "lgR_pred_list = []\n",
    "\n",
    "for iW in ts:\n",
    "    data_dir = '/g/data/w97/ad9701/p_prob_analysis/temp_files/'+varname+'_week'+str(iW)+'/'\n",
    "    ds = xr.open_mfdataset(data_dir + fname, chunks = {'lat':400, 'lon':400})\n",
    "    \n",
    "    # select predictors for the same period as the data\n",
    "    x1 = ds['time.season'].values     # season, this is the first predictor\n",
    "    da_time_bymon = np.array(pd.to_datetime(ds.time).to_period('M').to_timestamp().floor('D'))\n",
    "    ds_p_sel = ds_p.sel(time = da_time_bymon)\n",
    "    xp = []\n",
    "    for p in pNames:\n",
    "        xp.append(ds_p_sel[p].values)\n",
    "    xp_dict = dict(zip(pNames, xp))\n",
    "    xp_dict.update({\"season\": x1})    # add season to the sst predictors    \n",
    "    xp_df = pd.DataFrame(xp_dict)     # make a dataframe of predictors\n",
    "    \n",
    "    # create a new df of sample points at which 'predictions' will be made using the fitted model\n",
    "    newDf = createNewDf(xp_df, predSel)\n",
    "    \n",
    "    lgR_params = {}\n",
    "    lgR_pvalues = {}\n",
    "    lgR_pred = {}\n",
    "    \n",
    "    for iPt in np.arange(len(latList)):\n",
    "        latSel = latList[iPt]\n",
    "        lonSel = lonList[iPt]\n",
    "        da_pt = ds[vname].sel(lat = latSel, lon = lonSel).load()\n",
    "\n",
    "        for ith in np.arange(len(threshList)):\n",
    "            # field name to save the results\n",
    "            field = 'p'+str(iPt)+'_th'+str(ith)\n",
    "            \n",
    "            # create a dataframe of reponse and predictors\n",
    "            y = np.where(da_pt.values>=threshList[ith], 1, 0)\n",
    "            d = {\"response\": y}\n",
    "            if (sum(y)<4):\n",
    "                p_pred = newDf.copy()\n",
    "                p_pred['prob'] = 0\n",
    "                lgR_params.update({field:np.nan})\n",
    "                lgR_pvalues.update({field:np.nan})     \n",
    "            else:\n",
    "                d.update(xp_dict)\n",
    "                df = pd.DataFrame(d)\n",
    "\n",
    "                # fit the regression model\n",
    "                model = glm(formula, df, family=sm.families.Binomial())\n",
    "                model_GLM = model.fit()\n",
    "                p_pred = newDf.copy()\n",
    "                prob = model_GLM.predict(newDf)\n",
    "                p_pred['prob'] = prob\n",
    "            \n",
    "                # save the results\n",
    "                GLM_params = addLatLonThTimeDf(model_GLM.params, latSel, lonSel, threshList[ith], iW)\n",
    "                GLM_pvalues = addLatLonThTimeDf(model_GLM.pvalues, latSel, lonSel, threshList[ith], iW)\n",
    "                lgR_params.update({field:GLM_params})\n",
    "                lgR_pvalues.update({field:GLM_pvalues})\n",
    "            \n",
    "            GLM_pred = addLatLonThTimeDf(p_pred, latSel, lonSel, threshList[ith], iW)\n",
    "            lgR_pred.update({field:GLM_pred})\n",
    "\n",
    "    lgR_params_list.append(lgR_params)\n",
    "    lgR_pvalues_list.append(lgR_pvalues)\n",
    "    lgR_pred_list.append(lgR_pred)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0f586c6e-89e8-440b-8aaa-d53aaa2a99fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### save dataframes for plotting script\n",
    "import pickle\n",
    "# df.to_pickle(file_name)\n",
    "\n",
    "file_pvalues = out_dir + 'lgR_pvalues.pkl'\n",
    "file_params = out_dir + 'lgR_params.pkl'\n",
    "file_pred = out_dir + 'lgR_pred.pkl'\n",
    "\n",
    "import os, errno\n",
    "\n",
    "def silentremove(filename):\n",
    "    try:\n",
    "        os.remove(filename)\n",
    "    except OSError as e: # this would be \"except OSError, e:\" before Python 2.6\n",
    "        if e.errno != errno.ENOENT: # errno.ENOENT = no such file or directory\n",
    "            raise # re-raise exception if a different error occurred\n",
    "            \n",
    "silentremove(file_pvalues)\n",
    "silentremove(file_params)\n",
    "silentremove(file_pred)\n",
    "\n",
    "with open(file_pvalues, 'wb') as f:\n",
    "    pickle.dump(lgR_pvalues_list, f)\n",
    "\n",
    "with open(file_params, 'wb') as f:\n",
    "    pickle.dump(lgR_params_list, f)\n",
    "    \n",
    "with open(file_pred, 'wb') as f:\n",
    "    pickle.dump(lgR_pred_list, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:analysis3-21.07]",
   "language": "python",
   "name": "conda-env-analysis3-21.07-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
